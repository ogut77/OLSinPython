# -*- coding: utf-8 -*-
"""RegressionInPython.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HpyS3Q6moi190fVNvz0R9T-poTubEgkO
"""

import matplotlib.pyplot as plt
import matplotlib.pyplot as plt2
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
import pandas as pd 
import statsmodels.formula.api as smf
data1=pd.read_csv("https://hogut.weebly.com/uploads/1/8/1/6/18163409/cars.csv",delimiter=';')
#Let's see first five observation
print(data1[:5])

#We will take logarithmic transformation of variables and print 5 observation 
lndata1=np.log(data1)
print(lndata1[:5])

#Let's see descriptive statistics 
print(data1.describe())
print(lndata1.describe())

#plot for data1
plt.figure(1)
plt.scatter(data1.speed, data1.dist,  color='black')

#plot for lndata (log transformation)
plt.figure(2)
plt.scatter(lndata1.speed, lndata1.dist,  color='red')



#It seems 2nd graph is more suited for linear relation. 
#Let's see from regression results
results = smf.ols('dist ~ speed', data=data1).fit()
print(results.summary())

lnresults= smf.ols('dist ~ speed', data=lndata1).fit()
print(lnresults.summary())

#Log log model (2nd model) has higher R2, adj-R2 and F-stat. Thus, it is better
#Data points are more concentrated around predicted line at the following second graph
ypred = results.predict()
plt.figure(3)
plt.plot(data1.speed, ypred, color='blue', linewidth=3)
plt.scatter(data1.speed, data1.dist,  color='black')

lnypred = lnresults.predict()
plt.figure(4)
plt.scatter(lndata1.speed, lndata1.dist,  color='red')
plt.plot(lndata1.speed, lnypred, color='blue', linewidth=3)